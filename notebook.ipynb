{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.spaces import Box\n",
    "from IPython.display import clear_output\n",
    "from torch import nn, optim\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Env(gym.Wrapper):\n",
    "    def __init__(self, game_name):\n",
    "        env = gym.make(f\"{game_name}NoFrameskip-v4\")\n",
    "        super().__init__(env)\n",
    "        self.rep_memory = np.zeros((2, *env.observation_space.shape), np.uint8)\n",
    "        self.lives = 0\n",
    "        self.done = True\n",
    "        self.observation_space = Box(0, 1, (1, 84, 84), np.uint8)\n",
    "\n",
    "    def _reset(self, **kwargs):\n",
    "        if self.done:\n",
    "            self.env.reset(**kwargs)\n",
    "            # Test evaluation method (<= 30 random no-ops)\n",
    "            for _ in range(self.unwrapped.np_random.randint(1, 31)):\n",
    "                s, _, done, _ = self.env.step(0)\n",
    "                if done:\n",
    "                    s = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            done = self.done  # backup b/c step() overrides it\n",
    "            s, _, _, _ = self.env.step(0)  # re-spawn back to life\n",
    "            self.done = done\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self._reset(**kwargs)\n",
    "        if \"FIRE\" in self.unwrapped.get_action_meanings():\n",
    "            s, _, done, _ = self.step(1)  # Activate firing in environment\n",
    "            if done:\n",
    "                self._reset(**kwargs)\n",
    "            s, _, done, _ = self.step(2)\n",
    "            if done:\n",
    "                self._reset(**kwargs)\n",
    "        return s\n",
    "\n",
    "    def step(self, a):\n",
    "        reward = 0.0\n",
    "        for i in range(4):\n",
    "            s, r, done, info = self.env.step(a)\n",
    "            if i in (2, 3):\n",
    "                self.rep_memory[i - 2] = s\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "        s = self.rep_memory.max(0)\n",
    "\n",
    "        self.done = done\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        done = 0 < lives < self.lives or done\n",
    "        self.lives = lives\n",
    "        r = np.sign(reward)  # clip to [-1, 1]\n",
    "        return self.observation(s), r, done, info\n",
    "\n",
    "    def observation(self, s):\n",
    "        \"\"\"As in Nature paper\"\"\"\n",
    "        s = cv2.resize(cv2.cvtColor(s, cv2.COLOR_RGB2GRAY),\n",
    "                       (84, 84), interpolation=cv2.INTER_AREA)[:, :, None]\n",
    "        s = np.swapaxes(s, 2, 0)  # to feed into DQN\n",
    "        return s\n",
    "\n",
    "\n",
    "class PER:\n",
    "    \n",
    "    # prioritizing using a replay memory of 100000 transitions\n",
    "    n_transitions = 10**5\n",
    "    # factor that determines how much prioritization is applied\n",
    "    alpha = 0.6\n",
    "    # TODO add alpha, beta, n_transition and minibatch, we can play with them \n",
    "    def __init__(self):\n",
    "\n",
    "        self.priorities = np.zeros(self.n_transitions, np.float32)\n",
    "        self.i = 0 #  index\n",
    "        self.rep_memory = list()\n",
    "\n",
    "    def store(self, s, a, r, s_prime, done):\n",
    "        \"\"\"Store or add a transition to the Replay Memory\"\"\"\n",
    "        \n",
    "        s, s_prime = np.asarray([s]), np.asarray([s_prime])\n",
    "        memory_sz = len(self.rep_memory)\n",
    "\n",
    "        if memory_sz >= self.n_transitions:  # full\n",
    "            self.rep_memory[self.i] = (s, a, r, s_prime, done)\n",
    "        else:\n",
    "            self.rep_memory.append((s, a, r, s_prime, done))\n",
    "\n",
    "        if memory_sz == 0:\n",
    "            highest_priority = 1\n",
    "        elif memory_sz >= 1:\n",
    "            highest_priority = self.priorities.max()    \n",
    "\n",
    "        self.priorities[self.i] = highest_priority\n",
    "\n",
    "        self.i += 1  \n",
    "        self.i %= self.n_transitions\n",
    "\n",
    "    def replay(self, n_sample, beta):\n",
    "        \"\"\"replaying transitions following probabilty of sampling transition i\n",
    "        defined in equation (1) of the Prioritzed Experience Replay paper \"\"\"\n",
    "\n",
    "        memory_sz = len(self.rep_memory)\n",
    "\n",
    "        if memory_sz != self.n_transitions:\n",
    "            priorities = self.priorities[:self.i]\n",
    "        else:\n",
    "            priorities = self.priorities\n",
    "\n",
    "        # Probabilities of sampling transitions\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities = probabilities / probabilities.sum()\n",
    "\n",
    "        idx = np.random.choice(a=memory_sz, \n",
    "                                   size=n_sample, \n",
    "                                   replace=True, \n",
    "                                   p=probabilities)\n",
    "        \n",
    "        # compute importance-sampling weights\n",
    "        # from Algorithm 1 of PER paper\n",
    "        W = 1 / ( probabilities[idx] * memory_sz ) ** beta\n",
    "        W = W / W.max()\n",
    "\n",
    "        S, A, R, S_prime, Done = zip(*(self.rep_memory[i] for i in idx))\n",
    "        S, S_prime = map(np.concatenate, (S, S_prime))\n",
    "\n",
    "        return S, A, R, S_prime, Done, W, idx,\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    \"\"\"model architecture follows the paper\n",
    "    Human-level control through deep reinforcement learning\n",
    "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\"\"\"\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "    def action_selection(self, s, epsilon):\n",
    "\n",
    "        if epsilon >= np.random.random():\n",
    "            a = np.random.choice(self.n_actions)\n",
    "            return a\n",
    "        # else:? TODO check\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor([s], dtype=torch.float32, device=DEVICE)\n",
    "            Q_qnet = self(s)\n",
    "            a = Q_qnet.argmax(1).item()\n",
    "            return a\n",
    "\n",
    "\n",
    "def main(*game_names):\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "    T = 700_000\n",
    "\n",
    "    for game_name in game_names:\n",
    "        env = Env(game_name)\n",
    "        env.seed(1)\n",
    "\n",
    "        # double DQN, primary network qnet\n",
    "        qnet = DQN(env.action_space.n).to(DEVICE)\n",
    "        # the target network\n",
    "        tnet = DQN(env.action_space.n).to(DEVICE)\n",
    "        optimizer = optim.Adam(qnet.parameters(), lr=0.0001)\n",
    "\n",
    "        per = PER()\n",
    "        tnet.load_state_dict(qnet.state_dict())  # Synchronize policies\n",
    "\n",
    "        epsilon, k, gamma = 0.01, 32, 0.99\n",
    "        rewards, reward, plot_rewards = list(), 0, list()\n",
    "\n",
    "        s = env.reset()\n",
    "        for t in range(1, T + 1):\n",
    "            a = qnet.action_selection(s, epsilon)\n",
    "            s_prime, r, done, _ = env.step(a)\n",
    "            per.store(s, a, r, s_prime, done)\n",
    "            s = s_prime\n",
    "            reward += r\n",
    "            if done:\n",
    "                s = env.reset()\n",
    "                rewards.append(reward)\n",
    "                reward = 0\n",
    "            if len(per.rep_memory) > (1e4):\n",
    "                beta = min(1.0, 0.4 + t * 0.6 / (1e5) )  # Linear annealing\n",
    "                # TD Loss\n",
    "                S, A, R, S_prime, Done, W, idx = per.replay(\n",
    "                    k, beta)\n",
    "\n",
    "                S = torch.tensor(S, dtype=torch.float32,\n",
    "                                 requires_grad=True, device=DEVICE)\n",
    "                S_prime = torch.tensor(\n",
    "                    S_prime, dtype=torch.float32, requires_grad=True, device=DEVICE)\n",
    "                A = torch.tensor(A, dtype=torch.long, device=DEVICE)\n",
    "                R = torch.tensor(R, dtype=torch.long, device=DEVICE)\n",
    "                Done = torch.tensor(Done, dtype=torch.long, device=DEVICE)\n",
    "                W = torch.tensor(W, dtype=torch.long, device=DEVICE)\n",
    "                Q_qnet = qnet(S).gather(1, A.unsqueeze(1)).squeeze(1)\n",
    "                loss = (Q_qnet - (R + gamma * tnet(S_prime).max(1)\n",
    "                                  [0] * (1 - Done)).detach()) ** 2 * W\n",
    "                priorities = loss + 1e-5\n",
    "                loss = loss.mean()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                per.priorities[idx] = priorities.detach().cpu().numpy()\n",
    "                optimizer.step()\n",
    "\n",
    "            if t % (T / 200) == 0:\n",
    "                plot_rewards.append(np.mean(rewards[-10:]))\n",
    "                clear_output(True)\n",
    "                plt.figure(figsize=(12,8))\n",
    "                plt.plot(plot_rewards, 'g', linewidth=2, markersize=12)\n",
    "                plt.title(game_name)\n",
    "                plt.title(game_name, fontsize=14)\n",
    "                plt.xlabel(f\"Training step / {T // 200}\", fontsize=12)\n",
    "                plt.ylabel(\"reward\", fontsize=12)\n",
    "                plt.savefig(game_name + \".png\")\n",
    "                plt.show()\n",
    "\n",
    "            if t % (T / 10) == 0:\n",
    "                tnet.load_state_dict(qnet.state_dict())  # Synchronize policies\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "${KERNEL_SPEC_DISPLAY_NAME}",
   "language": "${KERNEL_SPEC_LANGUAGE}",
   "name": "${KERNEL_SPEC_NAME}"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}